@article{avantsAdvancedNormalizationTools,
  title = {Advanced {{Normalization Tools}} ({{ANTS}})},
  author = {Avants, Brian B and Tustison, Nick and Johnson, Hans},
  abstract = {We provide examples and highlights of Advanced Normalization Tools (ANTs), versions 2.x, that address practical problems in real data.},
  langid = {english},
  file = {/home/temple/Sync/zotero/storage/KT7YJXED/Avants et al. - Advanced Normalization Tools (ANTS).pdf}
}

@article{avantsSymmetricDiffeomorphicImage2008,
  title = {Symmetric Diffeomorphic Image Registration with Cross-Correlation: {{Evaluating}} Automated Labeling of Elderly and Neurodegenerative Brain},
  shorttitle = {Symmetric Diffeomorphic Image Registration with Cross-Correlation},
  author = {Avants, B and Epstein, C and Grossman, M and Gee, J},
  year = {2008},
  month = feb,
  journal = {Medical Image Analysis},
  volume = {12},
  number = {1},
  pages = {26--41},
  issn = {13618415},
  doi = {10.1016/j.media.2007.06.004},
  urldate = {2024-05-03},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/home/temple/Sync/zotero/storage/NRGQ7FAQ/Avants et al. - 2008 - Symmetric diffeomorphic image registration with cr.pdf}
}

@article{balakrishnanVoxelMorphLearningFramework2018,
  title = {{{VoxelMorph}}: {{A Learning Framework}} for {{Deformable Medical Image Registration}}},
  shorttitle = {{{VoxelMorph}}},
  author = {Balakrishnan, Guha and Zhao, Amy and Sabuncu, Mert R. and Guttag, John and Dalca, Adrian V.},
  year = {2018},
  publisher = {[object Object]},
  doi = {10.48550/ARXIV.1809.05231},
  urldate = {2024-05-02},
  abstract = {We present VoxelMorph, a fast learning-based framework for deformable, pairwise medical image registration. Traditional registration methods optimize an objective function for each pair of images, which can be time-consuming for large datasets or rich deformation models. In contrast to this approach, and building on recent learning-based methods, we formulate registration as a function that maps an input image pair to a deformation field that aligns these images. We parameterize the function via a convolutional neural network (CNN), and optimize the parameters of the neural network on a set of images. Given a new pair of scans, VoxelMorph rapidly computes a deformation field by directly evaluating the function. In this work, we explore two different training strategies. In the first (unsupervised) setting, we train the model to maximize standard image matching objective functions that are based on the image intensities. In the second setting, we leverage auxiliary segmentations available in the training data. We demonstrate that the unsupervised model's accuracy is comparable to state-of-the-art methods, while operating orders of magnitude faster. We also show that VoxelMorph trained with auxiliary data improves registration accuracy at test time, and evaluate the effect of training set size on registration. Our method promises to speed up medical image analysis and processing pipelines, while facilitating novel directions in learning-based registration and its applications. Our code is freely available at voxelmorph.csail.mit.edu.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences},
  file = {/home/temple/Sync/zotero/storage/6IWELYZI/Balakrishnan et al. - 2018 - VoxelMorph A Learning Framework for Deformable Me.pdf}
}

@misc{battagliaRelationalInductiveBiases2018,
  title = {Relational Inductive Biases, Deep Learning, and Graph Networks},
  author = {Battaglia, Peter W. and Hamrick, Jessica B. and Bapst, Victor and {Sanchez-Gonzalez}, Alvaro and Zambaldi, Vinicius and Malinowski, Mateusz and Tacchetti, Andrea and Raposo, David and Santoro, Adam and Faulkner, Ryan and Gulcehre, Caglar and Song, Francis and Ballard, Andrew and Gilmer, Justin and Dahl, George and Vaswani, Ashish and Allen, Kelsey and Nash, Charles and Langston, Victoria and Dyer, Chris and Heess, Nicolas and Wierstra, Daan and Kohli, Pushmeet and Botvinick, Matt and Vinyals, Oriol and Li, Yujia and Pascanu, Razvan},
  year = {2018},
  month = oct,
  number = {arXiv:1806.01261},
  eprint = {1806.01261},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-06-20},
  abstract = {Artificial intelligence (AI) has undergone a renaissance recently, making major progress in key domains such as vision, language, control, and decision-making. This has been due, in part, to cheap data and cheap compute resources, which have fit the natural strengths of deep learning. However, many defining characteristics of human intelligence, which developed under much different pressures, remain out of reach for current approaches. In particular, generalizing beyond one's experiences--a hallmark of human intelligence from infancy--remains a formidable challenge for modern AI. The following is part position paper, part review, and part unification. We argue that combinatorial generalization must be a top priority for AI to achieve human-like abilities, and that structured representations and computations are key to realizing this objective. Just as biology uses nature and nurture cooperatively, we reject the false choice between "hand-engineering" and "end-to-end" learning, and instead advocate for an approach which benefits from their complementary strengths. We explore how using relational inductive biases within deep learning architectures can facilitate learning about entities, relations, and rules for composing them. We present a new building block for the AI toolkit with a strong relational inductive bias--the graph network--which generalizes and extends various approaches for neural networks that operate on graphs, and provides a straightforward interface for manipulating structured knowledge and producing structured behaviors. We discuss how graph networks can support relational reasoning and combinatorial generalization, laying the foundation for more sophisticated, interpretable, and flexible patterns of reasoning. As a companion to this paper, we have released an open-source software library for building graph networks, with demonstrations of how to use them in practice.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/temple/Sync/zotero/storage/BRPE7R4B/Battaglia et al. - 2018 - Relational inductive biases, deep learning, and gr.pdf;/home/temple/Sync/zotero/storage/IEMYSYK4/1806.html}
}

@article{chenTransMorphTransformerUnsupervised2021,
  title = {{{TransMorph}}: {{Transformer}} for Unsupervised Medical Image Registration},
  shorttitle = {{{TransMorph}}},
  author = {Chen, Junyu and Frey, Eric C. and He, Yufan and Segars, William P. and Li, Ye and Du, Yong},
  year = {2021},
  publisher = {[object Object]},
  doi = {10.48550/ARXIV.2111.10480},
  urldate = {2024-05-02},
  abstract = {In the last decade, convolutional neural networks (ConvNets) have been a major focus of research in medical image analysis. However, the performances of ConvNets may be limited by a lack of explicit consideration of the long-range spatial relationships in an image. Recently Vision Transformer architectures have been proposed to address the shortcomings of ConvNets and have produced state-of-the-art performances in many medical imaging applications. Transformers may be a strong candidate for image registration because their substantially larger receptive field enables a more precise comprehension of the spatial correspondence between moving and fixed images. Here, we present TransMorph, a hybrid Transformer-ConvNet model for volumetric medical image registration. This paper also presents diffeomorphic and Bayesian variants of TransMorph: the diffeomorphic variants ensure the topology-preserving deformations, and the Bayesian variant produces a well-calibrated registration uncertainty estimate. We extensively validated the proposed models using 3D medical images from three applications: inter-patient and atlas-to-patient brain MRI registration and phantom-to-CT registration. The proposed models are evaluated in comparison to a variety of existing registration methods and Transformer architectures. Qualitative and quantitative results demonstrate that the proposed Transformer-based model leads to a substantial performance improvement over the baseline methods, confirming the effectiveness of Transformers for medical image registration.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Artificial Intelligence (cs.AI),Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences,FOS: Electrical engineering electronic engineering information engineering,Image and Video Processing (eess.IV)},
  file = {/home/temple/Sync/zotero/storage/UKXNBZ9B/Chen et al. - 2021 - TransMorph Transformer for unsupervised medical i.pdf}
}

@article{chenViTVNetVisionTransformer2021,
  title = {{{ViT-V-Net}}: {{Vision Transformer}} for {{Unsupervised Volumetric Medical Image Registration}}},
  shorttitle = {{{ViT-V-Net}}},
  author = {Chen, Junyu and He, Yufan and Frey, Eric C. and Li, Ye and Du, Yong},
  year = {2021},
  publisher = {[object Object]},
  doi = {10.48550/ARXIV.2104.06468},
  urldate = {2024-05-07},
  abstract = {In the last decade, convolutional neural networks (ConvNets) have dominated and achieved state-of-the-art performances in a variety of medical imaging applications. However, the performances of ConvNets are still limited by lacking the understanding of long-range spatial relations in an image. The recently proposed Vision Transformer (ViT) for image classification uses a purely self-attention-based model that learns long-range spatial relations to focus on the relevant parts of an image. Nevertheless, ViT emphasizes the low-resolution features because of the consecutive downsamplings, result in a lack of detailed localization information, making it unsuitable for image registration. Recently, several ViT-based image segmentation methods have been combined with ConvNets to improve the recovery of detailed localization information. Inspired by them, we present ViT-V-Net, which bridges ViT and ConvNet to provide volumetric medical image registration. The experimental results presented here demonstrate that the proposed architecture achieves superior performance to several top-performing registration methods.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences,FOS: Electrical engineering electronic engineering information engineering,Image and Video Processing (eess.IV)},
  file = {/home/temple/Sync/zotero/storage/BWWH9PLZ/Chen et al. - 2021 - ViT-V-Net Vision Transformer for Unsupervised Vol.pdf}
}

@misc{cordonnierRelationshipSelfAttentionConvolutional2019,
  title = {On the {{Relationship}} between {{Self-Attention}} and {{Convolutional Layers}}},
  author = {Cordonnier, Jean-Baptiste and Loukas, Andreas and Jaggi, Martin},
  year = {2019},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.1911.03584},
  urldate = {2024-05-27},
  abstract = {Recent trends of incorporating attention mechanisms in vision have led researchers to reconsider the supremacy of convolutional layers as a primary building block. Beyond helping CNNs to handle long-range dependencies, Ramachandran et al. (2019) showed that attention can completely replace convolution and achieve state-of-the-art performance on vision tasks. This raises the question: do learned attention layers operate similarly to convolutional layers? This work provides evidence that attention layers can perform convolution and, indeed, they often learn to do so in practice. Specifically, we prove that a multi-head self-attention layer with sufficient number of heads is at least as expressive as any convolutional layer. Our numerical experiments then show that self-attention layers attend to pixel-grid patterns similarly to CNN layers, corroborating our analysis. Our code is publicly available.},
  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International},
  keywords = {Computation and Language (cs.CL),Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences,Machine Learning (cs.LG),Machine Learning (stat.ML)},
  file = {/home/temple/Sync/zotero/storage/R4MDARMU/Cordonnier et al. - 2019 - On the Relationship between Self-Attention and Con.pdf}
}

@misc{daiTransformerXLAttentiveLanguage2019,
  title = {Transformer-{{XL}}: {{Attentive Language Models Beyond}} a {{Fixed-Length Context}}},
  shorttitle = {Transformer-{{XL}}},
  author = {Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime and Le, Quoc V. and Salakhutdinov, Ruslan},
  year = {2019},
  month = jun,
  number = {arXiv:1901.02860},
  eprint = {1901.02860},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-06-20},
  abstract = {Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80\% longer than RNNs and 450\% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/temple/Sync/zotero/storage/T65GRBD8/Dai et al. - 2019 - Transformer-XL Attentive Language Models Beyond a.pdf;/home/temple/Sync/zotero/storage/L38JDT6N/1901.html}
}

@article{dehouwerWhatLearningNature2013,
  title = {What Is Learning? {{On}} the Nature and Merits of a Functional Definition of Learning},
  shorttitle = {What Is Learning?},
  author = {De Houwer, Jan and {Barnes-Holmes}, Dermot and Moors, Agnes},
  year = {2013},
  month = aug,
  journal = {Psychonomic Bulletin \& Review},
  volume = {20},
  number = {4},
  pages = {631--642},
  issn = {1069-9384, 1531-5320},
  doi = {10.3758/s13423-013-0386-3},
  urldate = {2024-05-01},
  copyright = {http://www.springer.com/tdm},
  langid = {english},
  file = {/home/temple/Sync/zotero/storage/DAVZKACK/De Houwer et al. - 2013 - What is learning On the nature and merits of a fu.pdf}
}

@article{dosovitskiyImageWorth16x162020,
  title = {An {{Image}} Is {{Worth}} 16x16 {{Words}}: {{Transformers}} for {{Image Recognition}} at {{Scale}}},
  shorttitle = {An {{Image}} Is {{Worth}} 16x16 {{Words}}},
  author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  year = {2020},
  publisher = {[object Object]},
  doi = {10.48550/ARXIV.2010.11929},
  urldate = {2024-05-03},
  abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Artificial Intelligence (cs.AI),Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences,Machine Learning (cs.LG)},
  file = {/home/temple/Sync/zotero/storage/Y4SYFQE8/Dosovitskiy et al. - 2020 - An Image is Worth 16x16 Words Transformers for Im.pdf}
}

@article{ferranteWeaklySupervisedLearningMetric2019,
  title = {Weakly-{{Supervised Learning}} of {{Metric Aggregations}} for {{Deformable Image Registration}}},
  author = {Ferrante, Enzo and Dokania, Puneet K. and Silva, Rafael Marini and Paragios, Nikos},
  year = {2019},
  month = jul,
  journal = {IEEE Journal of Biomedical and Health Informatics},
  volume = {23},
  number = {4},
  eprint = {1809.09004},
  primaryclass = {cs},
  pages = {1374--1384},
  issn = {2168-2194, 2168-2208},
  doi = {10.1109/JBHI.2018.2869700},
  urldate = {2024-05-29},
  abstract = {Deformable registration has been one of the pillars of biomedical image computing. Conventional approaches refer to the definition of a similarity criterion that, once endowed with a deformation model and a smoothness constraint, determines the optimal transformation to align two given images. The definition of this metric function is among the most critical aspects of the registration process. We argue that incorporating semantic information (in the form of anatomical segmentation maps) into the registration process will further improve the accuracy of the results. In this paper, we propose a novel weakly supervised approach to learn domain specific aggregations of conventional metrics using anatomical segmentations. This combination is learned using latent structured support vector machines (LSSVM). The learned matching criterion is integrated within a metric free optimization framework based on graphical models, resulting in a multi-metric algorithm endowed with a spatially varying similarity metric function conditioned on the anatomical structures. We provide extensive evaluation on three different datasets of CT and MRI images, showing that learned multi-metric registration outperforms single-metric approaches based on conventional similarity measures.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/temple/Sync/zotero/storage/AWWHHWXR/Ferrante et al. - 2019 - Weakly-Supervised Learning of Metric Aggregations .pdf;/home/temple/Sync/zotero/storage/G5KIME6M/1809.html}
}

@article{fuDeepLearningMedical2019,
  title = {Deep {{Learning}} in {{Medical Image Registration}}: {{A Review}}},
  shorttitle = {Deep {{Learning}} in {{Medical Image Registration}}},
  author = {Fu, Yabo and Lei, Yang and Wang, Tonghe and Curran, Walter J. and Liu, Tian and Yang, Xiaofeng},
  year = {2019},
  publisher = {[object Object]},
  doi = {10.48550/ARXIV.1912.12318},
  urldate = {2024-05-03},
  abstract = {This paper presents a review of deep learning (DL) based medical image registration methods. We summarized the latest developments and applications of DL-based registration methods in the medical field. These methods were classified into seven categories according to their methods, functions and popularity. A detailed review of each category was presented, highlighting important contributions and identifying specific challenges. A short assessment was presented following the detailed review of each category to summarize its achievements and future potentials. We provided a comprehensive comparison among DL-based methods for lung and brain deformable registration using benchmark datasets. Lastly, we analyzed the statistics of all the cited works from various aspects, revealing the popularity and future trend of development in medical image registration using deep learning.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences,FOS: Electrical engineering electronic engineering information engineering,FOS: Physical sciences,Image and Video Processing (eess.IV),Machine Learning (cs.LG),Machine Learning (stat.ML),Medical Physics (physics.med-ph)},
  file = {/home/temple/Sync/zotero/storage/MPGR7ZCS/Fu et al. - 2019 - Deep Learning in Medical Image Registration A Rev.pdf}
}

@article{gilesDynamicRecurrentNeural1994,
  title = {Dynamic Recurrent Neural Networks: {{Theory}} and Applications},
  shorttitle = {Dynamic Recurrent Neural Networks},
  author = {Giles, C. Lee and Kuhn, Gary M. and Williams, Ronald J.},
  year = {1994},
  month = mar,
  journal = {IEEE Transactions on Neural Networks},
  volume = {5},
  number = {2},
  pages = {153--156},
  issn = {1045-9227, 1941-0093},
  doi = {10.1109/TNN.1994.8753425},
  urldate = {2024-05-04},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  file = {/home/temple/Sync/zotero/storage/RXB97QQ7/Giles et al. - 1994 - Dynamic recurrent neural networks Theory and appl.pdf}
}

@book{goodfellowDeepLearning2016,
  title = {Deep Learning},
  author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year = {2016},
  series = {Adaptive Computation and Machine Learning},
  publisher = {The MIT press},
  address = {Cambridge, Mass},
  isbn = {978-0-262-03561-3},
  langid = {english},
  lccn = {006.31}
}

@article{hammoudehDeepLearningMedical2023,
  title = {Deep Learning in Medical Image Registration: Introduction and Survey},
  shorttitle = {Deep Learning in Medical Image Registration},
  author = {Hammoudeh, Ahmad and Dupont, St{\'e}phane},
  year = {2023},
  publisher = {[object Object]},
  doi = {10.48550/ARXIV.2309.00727},
  urldate = {2024-05-03},
  abstract = {Image registration (IR) is a process that deforms images to align them with respect to a reference space, making it easier for medical practitioners to examine various medical images in a standardized reference frame, such as having the same rotation and scale. This document introduces image registration using a simple numeric example. It provides a definition of image registration along with a space-oriented symbolic representation. This review covers various aspects of image transformations, including affine, deformable, invertible, and bidirectional transformations, as well as medical image registration algorithms such as Voxelmorph, Demons, SyN, Iterative Closest Point, and SynthMorph. It also explores atlas-based registration and multistage image registration techniques, including coarse-fine and pyramid approaches. Furthermore, this survey paper discusses medical image registration taxonomies, datasets, evaluation measures, such as correlation-based metrics, segmentation-based metrics, processing time, and model size. It also explores applications in image-guided surgery, motion tracking, and tumor diagnosis. Finally, the document addresses future research directions, including the further development of transformers.},
  copyright = {Creative Commons Attribution 4.0 International},
  langid = {english},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences,FOS: Electrical engineering electronic engineering information engineering,Image and Video Processing (eess.IV),Machine Learning (cs.LG)},
  file = {/home/temple/Sync/zotero/storage/FEPLAUA9/Hammoudeh and Dupont - 2023 - Deep learning in medical image registration intro.pdf}
}

@article{heDeepResidualLearning2015,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2015},
  publisher = {[object Object]},
  doi = {10.48550/ARXIV.1512.03385},
  urldate = {2024-05-04},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \&amp; COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences},
  file = {/home/temple/Sync/zotero/storage/I8KDE8NG/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf}
}

@article{hochreiterLongShortTermMemory1997,
  title = {Long {{Short-Term Memory}}},
  author = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  year = {1997},
  month = nov,
  journal = {Neural Computation},
  volume = {9},
  number = {8},
  pages = {1735--1780},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/neco.1997.9.8.1735},
  urldate = {2024-06-18},
  abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
  langid = {english},
  file = {/home/temple/Sync/zotero/storage/ST6YR4YP/Hochreiter and Schmidhuber - 1997 - Long Short-Term Memory.pdf}
}

@article{hornikMultilayerFeedforwardNetworks1989,
  title = {Multilayer Feedforward Networks Are Universal Approximators},
  author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
  year = {1989},
  month = jan,
  journal = {Neural Networks},
  volume = {2},
  number = {5},
  pages = {359--366},
  issn = {08936080},
  doi = {10.1016/0893-6080(89)90020-8},
  urldate = {2024-05-31},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/home/temple/Sync/zotero/storage/R42228MQ/Hornik et al. - 1989 - Multilayer feedforward networks are universal appr.pdf}
}

@article{jaderbergSpatialTransformerNetworks2015,
  title = {Spatial {{Transformer Networks}}},
  author = {Jaderberg, Max and Simonyan, Karen and Zisserman, Andrew and Kavukcuoglu, Koray},
  year = {2015},
  publisher = {[object Object]},
  doi = {10.48550/ARXIV.1506.02025},
  urldate = {2024-05-03},
  abstract = {Convolutional Neural Networks define an exceptionally powerful class of models, but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner. In this work we introduce a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network. This differentiable module can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation process. We show that the use of spatial transformers results in models which learn invariance to translation, scale, rotation and more generic warping, resulting in state-of-the-art performance on several benchmarks, and for a number of classes of transformations.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences},
  file = {/home/temple/Sync/zotero/storage/X2SZLA5Q/Jaderberg et al. - 2015 - Spatial Transformer Networks.pdf}
}

@article{kimCycleMorphCycleConsistent2020,
  title = {{{CycleMorph}}: {{Cycle Consistent Unsupervised Deformable Image Registration}}},
  shorttitle = {{{CycleMorph}}},
  author = {Kim, Boah and Kim, Dong Hwan and Park, Seong Ho and Kim, Jieun and Lee, June-Goo and Ye, Jong Chul},
  year = {2020},
  publisher = {[object Object]},
  doi = {10.48550/ARXIV.2008.05772},
  urldate = {2024-05-02},
  abstract = {Image registration is a fundamental task in medical image analysis. Recently, deep learning based image registration methods have been extensively investigated due to their excellent performance despite the ultra-fast computational time. However, the existing deep learning methods still have limitation in the preservation of original topology during the deformation with registration vector fields. To address this issues, here we present a cycle-consistent deformable image registration. The cycle consistency enhances image registration performance by providing an implicit regularization to preserve topology during the deformation. The proposed method is so flexible that can be applied for both 2D and 3D registration problems for various applications, and can be easily extended to multi-scale implementation to deal with the memory issues in large volume registration. Experimental results on various datasets from medical and non-medical applications demonstrate that the proposed method provides effective and accurate registration on diverse image pairs within a few seconds. Qualitative and quantitative evaluations on deformation fields also verify the effectiveness of the cycle consistency of the proposed method.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences,FOS: Electrical engineering electronic engineering information engineering,Image and Video Processing (eess.IV),Machine Learning (cs.LG),Machine Learning (stat.ML)},
  file = {/home/temple/Sync/zotero/storage/FGMRRIB9/Kim et al. - 2020 - CycleMorph Cycle Consistent Unsupervised Deformab.pdf;/home/temple/Sync/zotero/storage/HNM6HHY3/ants2.pdf}
}

@misc{kovalevaRevealingDarkSecrets2019,
  title = {Revealing the {{Dark Secrets}} of {{BERT}}},
  author = {Kovaleva, Olga and Romanov, Alexey and Rogers, Anna and Rumshisky, Anna},
  year = {2019},
  month = sep,
  number = {arXiv:1908.08593},
  eprint = {1908.08593},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-06-20},
  abstract = {BERT-based architectures currently give state-of-the-art performance on many NLP tasks, but little is known about the exact mechanisms that contribute to its success. In the current work, we focus on the interpretation of self-attention, which is one of the fundamental underlying components of BERT. Using a subset of GLUE tasks and a set of handcrafted features-of-interest, we propose the methodology and carry out a qualitative and quantitative analysis of the information encoded by the individual BERT's heads. Our findings suggest that there is a limited set of attention patterns that are repeated across different heads, indicating the overall model overparametrization. While different heads consistently use the same attention patterns, they have varying impact on performance across different tasks. We show that manually disabling attention in certain heads leads to a performance improvement over the regular fine-tuned BERT models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/temple/Sync/zotero/storage/MBVHRFJ9/Kovaleva et al. - 2019 - Revealing the Dark Secrets of BERT.pdf;/home/temple/Sync/zotero/storage/SEMQKP6V/1908.html}
}

@article{lecunDeepLearning2015,
  title = {Deep Learning},
  author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  year = {2015},
  month = may,
  journal = {Nature},
  volume = {521},
  number = {7553},
  pages = {436--444},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature14539},
  urldate = {2024-05-03},
  langid = {english},
  file = {/home/temple/Sync/zotero/storage/GHK2WR8X/LeCun et al. - 2015 - Deep learning.pdf}
}

@article{liuSwinTransformerHierarchical2021,
  title = {Swin {{Transformer}}: {{Hierarchical Vision Transformer}} Using {{Shifted Windows}}},
  shorttitle = {Swin {{Transformer}}},
  author = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  year = {2021},
  publisher = {[object Object]},
  doi = {10.48550/ARXIV.2103.14030},
  urldate = {2024-05-03},
  abstract = {This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with {\textbackslash}textbf\{S\}hifted {\textbackslash}textbf\{win\}dows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures. The code and models are publicly available at{\textasciitilde}{\textbackslash}url\{https://github.com/microsoft/Swin-Transformer\}.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences,Machine Learning (cs.LG)},
  file = {/home/temple/Sync/zotero/storage/EA9FSMBR/Liu et al. - 2021 - Swin Transformer Hierarchical Vision Transformer .pdf}
}

@article{maSymmetricTransformerbasedNetwork2022,
  title = {Symmetric {{Transformer-based Network}} for {{Unsupervised Image Registration}}},
  author = {Ma, Mingrui and Song, Lei and Xu, Yuanbo and Liu, Guixia},
  year = {2022},
  publisher = {[object Object]},
  doi = {10.48550/ARXIV.2204.13575},
  urldate = {2024-05-03},
  abstract = {Medical image registration is a fundamental and critical task in medical image analysis. With the rapid development of deep learning, convolutional neural networks (CNN) have dominated the medical image registration field. Due to the disadvantage of the local receptive field of CNN, some recent registration methods have focused on using transformers for non-local registration. However, the standard Transformer has a vast number of parameters and high computational complexity, which causes Transformer can only be applied at the bottom of the registration models. As a result, only coarse information is available at the lowest resolution, limiting the contribution of Transformer in their models. To address these challenges, we propose a convolution-based efficient multi-head self-attention (CEMSA) block, which reduces the parameters of the traditional Transformer and captures local spatial context information for reducing semantic ambiguity in the attention mechanism. Based on the proposed CEMSA, we present a novel Symmetric Transformer-based model (SymTrans). SymTrans employs the Transformer blocks in the encoder and the decoder respectively to model the long-range spatial cross-image relevance. We apply SymTrans to the displacement field and diffeomorphic registration. Experimental results show that our proposed method achieves state-of-the-art performance in image registration. Our code is publicly available at {\textbackslash}url\{https://github.com/MingR-Ma/SymTrans\}.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences},
  file = {/home/temple/Sync/zotero/storage/QCKYVUQ9/Ma et al. - 2022 - Symmetric Transformer-based Network for Unsupervis.pdf}
}

@article{milletariVNetFullyConvolutional2016,
  title = {V-{{Net}}: {{Fully Convolutional Neural Networks}} for {{Volumetric Medical Image Segmentation}}},
  shorttitle = {V-{{Net}}},
  author = {Milletari, Fausto and Navab, Nassir and Ahmadi, Seyed-Ahmad},
  year = {2016},
  publisher = {[object Object]},
  doi = {10.48550/ARXIV.1606.04797},
  urldate = {2024-05-03},
  abstract = {Convolutional Neural Networks (CNNs) have been recently employed to solve problems from both the computer vision and medical image analysis fields. Despite their popularity, most approaches are only able to process 2D images while most medical data used in clinical practice consists of 3D volumes. In this work we propose an approach to 3D image segmentation based on a volumetric, fully convolutional, neural network. Our CNN is trained end-to-end on MRI volumes depicting prostate, and learns to predict segmentation for the whole volume at once. We introduce a novel objective function, that we optimise during training, based on Dice coefficient. In this way we can deal with situations where there is a strong imbalance between the number of foreground and background voxels. To cope with the limited number of annotated volumes available for training, we augment the data applying random non-linear transformations and histogram matching. We show in our experimental evaluation that our approach achieves good performances on challenging test data while requiring only a fraction of the processing time needed by other previous methods.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences},
  file = {/home/temple/Sync/zotero/storage/QACT6FM8/Milletari et al. - 2016 - V-Net Fully Convolutional Neural Networks for Vol.pdf}
}

@article{mitchellDisciplineMachineLearning2006,
  title = {The {{Discipline}} of {{Machine Learning}}},
  author = {Mitchell, Tom M},
  year = {2006},
  month = jul,
  pages = {9},
  abstract = {Over the past 50 years the study of Machine Learning has grown from the efforts of a handful of computer engineers exploring whether computers could learn to play games, and a field of Statistics that largely ignored computational considerations, to a broad discipline that has produced fundamental statistical-computational theories of learning processes, has designed learning algorithms that are routinely used in commercial systems for speech recognition, computer vision, and a variety of other tasks, and has spun off an industry in data mining to discover hidden regularities in the growing volumes of online data. This document provides a brief and personal view of the discipline that has emerged as Machine Learning, the fundamental questions it addresses, its relationship to other sciences and society, and where it might be headed.},
  langid = {english},
  file = {/home/temple/Sync/zotero/storage/G4CSKKME/Mitchell - The Discipline of Machine Learning.pdf}
}

@inproceedings{NIPS2012_c399862d,
  title = {{{ImageNet}} Classification with Deep Convolutional Neural Networks},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  editor = {Pereira, F. and Burges, C.J. and Bottou, L. and Weinberger, K.Q.},
  year = {2012},
  volume = {25},
  publisher = {Curran Associates, Inc.},
  file = {/home/temple/Sync/zotero/storage/LH7EY45G/Krizhevsky et al. - 2012 - ImageNet classification with deep convolutional ne.pdf}
}

@article{palPositiveJacobianLearn2022,
  title = {Towards {{Positive Jacobian}}: {{Learn}} to {{Postprocess Diffeomorphic Image Registration}} with {{Matrix Exponential}}},
  shorttitle = {Towards {{Positive Jacobian}}},
  author = {Pal, Soumyadeep and Tennant, Matthew and Ray, Nilanjan},
  year = {2022},
  publisher = {[object Object]},
  doi = {10.48550/ARXIV.2202.00749},
  urldate = {2024-05-04},
  abstract = {We present a postprocessing layer for deformable image registration to make a registration field more diffeomorphic by encouraging Jacobians of the transformation to be positive. Diffeomorphic image registration is important for medical imaging studies because of the properties like invertibility, smoothness of the transformation, and topology preservation/non-folding of the grid. Violation of these properties can lead to destruction of the neighbourhood and the connectivity of anatomical structures during image registration. Most of the recent deep learning methods do not explicitly address this folding problem and try to solve it with a smoothness regularization on the registration field. In this paper, we propose a differentiable layer, which takes any registration field as its input, computes exponential of the Jacobian matrices of the input and reconstructs a new registration field from the exponentiated Jacobian matrices using Poisson reconstruction. Our proposed Poisson reconstruction loss enforces positive Jacobians for the final registration field. Thus, our method acts as a post-processing layer without any learnable parameters of its own and can be placed at the end of any deep learning pipeline to form an end-to-end learnable framework. We show the effectiveness of our proposed method for a popular deep learning registration method Voxelmorph and evaluate it with a dataset containing 3D brain MRI scans. Our results show that our post-processing can effectively decrease the number of non-positive Jacobians by a significant amount without any noticeable deterioration of the registration accuracy, thus making the registration field more diffeomorphic. Our code is available online at https://github.com/Soumyadeep-Pal/Diffeomorphic-Image-Registration-Postprocess.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences,FOS: Electrical engineering electronic engineering information engineering,Image and Video Processing (eess.IV)},
  file = {/home/temple/Sync/zotero/storage/6XMMZVRD/Pal et al. - 2022 - Towards Positive Jacobian Learn to Postprocess Di.pdf}
}

@incollection{ronnebergerUNetConvolutionalNetworks2015,
  title = {U-{{Net}}: {{Convolutional Networks}} for {{Biomedical Image Segmentation}}},
  shorttitle = {U-{{Net}}},
  booktitle = {Medical {{Image Computing}} and {{Computer-Assisted Intervention}} -- {{MICCAI}} 2015},
  author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  editor = {Navab, Nassir and Hornegger, Joachim and Wells, William M. and Frangi, Alejandro F.},
  year = {2015},
  volume = {9351},
  pages = {234--241},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-24574-4_28},
  urldate = {2024-05-03},
  isbn = {978-3-319-24573-7 978-3-319-24574-4},
  langid = {english},
  file = {/home/temple/Sync/zotero/storage/K5ER8ND3/Ronneberger et al. - 2015 - U-Net Convolutional Networks for Biomedical Image.pdf}
}

@misc{serranoAttentionInterpretable2019,
  title = {Is {{Attention Interpretable}}?},
  author = {Serrano, Sofia and Smith, Noah A.},
  year = {2019},
  month = jun,
  number = {arXiv:1906.03731},
  eprint = {1906.03731},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-06-20},
  abstract = {Attention mechanisms have recently boosted performance on a range of NLP tasks. Because attention layers explicitly weight input components' representations, it is also often assumed that attention can be used to identify information that models found important (e.g., specific contextualized word tokens). We test whether that assumption holds by manipulating attention weights in already-trained text classification models and analyzing the resulting differences in their predictions. While we observe some ways in which higher attention weights correlate with greater impact on model predictions, we also find many ways in which this does not hold, i.e., where gradient-based rankings of attention weights better predict their effects than their magnitudes. We conclude that while attention noisily predicts input components' overall importance to a model, it is by no means a fail-safe indicator.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/temple/Sync/zotero/storage/5J5MQ62T/Serrano and Smith - 2019 - Is Attention Interpretable.pdf;/home/temple/Sync/zotero/storage/VS2XIYJU/1906.html}
}

@misc{sutskeverSequenceSequenceLearning2014,
  title = {Sequence to {{Sequence Learning}} with {{Neural Networks}}},
  author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
  year = {2014},
  month = dec,
  number = {arXiv:1409.3215},
  eprint = {1409.3215},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-06-20},
  abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/temple/Sync/zotero/storage/4TNLTGLN/Sutskever et al. - 2014 - Sequence to Sequence Learning with Neural Networks.pdf;/home/temple/Sync/zotero/storage/9TUDRNES/1409.html}
}

@article{vaswaniAttentionAllYou2017,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year = {2017},
  publisher = {[object Object]},
  doi = {10.48550/ARXIV.1706.03762},
  urldate = {2024-05-03},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Computation and Language (cs.CL),FOS: Computer and information sciences,Machine Learning (cs.LG)},
  file = {/home/temple/Sync/zotero/storage/CWMSTQU9/Vaswani et al. - 2017 - Attention Is All You Need.pdf}
}

@article{voskrebenzevFeasibilityQuantitativeRegional2018,
  title = {Feasibility of Quantitative Regional Ventilation and Perfusion Mapping with Phase-resolved Functional Lung ({{PREFUL}}) {{MRI}} in Healthy Volunteers and {{COPD}}, {{CTEPH}}, and {{CF}} Patients},
  author = {Voskrebenzev, Andreas and Gutberlet, Marcel and Klime{\v s}, Filip and Kaireit, Till F. and Sch{\"o}nfeld, Christian and Rot{\"a}rmel, Alexander and Wacker, Frank and Vogel-Claussen, Jens},
  year = {2018},
  month = apr,
  journal = {Magnetic Resonance in Medicine},
  volume = {79},
  number = {4},
  pages = {2306--2314},
  issn = {0740-3194, 1522-2594},
  doi = {10.1002/mrm.26893},
  urldate = {2024-05-03},
  abstract = {Purpose               In this feasibility study, a phase-resolved functional lung imaging postprocessing method for extraction of dynamic perfusion (Q) and ventilation (V) parameters using a conventional 1H lung MRI Fourier decomposition acquisition is introduced.                                         Methods               Time series of coronal gradient-echo MR images with a temporal resolution of 288 to 324 ms of two healthy volunteers, one patient with chronic thromboembolic hypertension, one patient with cystic fibrosis, and one patient with chronic obstructive pulmonary disease were acquired at 1.5 T. Using a sine model to estimate cardiac and respiratory phases of each image, all images were sorted to reconstruct full cardiac and respiratory cycles. Time to peak (TTP), V/Q maps, and fractional ventilation flow-volume loops were calculated.                                         Results               For the volunteers, homogenous ventilation and perfusion TTP maps (V-TTP, Q-TTP) were obtained. The chronic thromboembolic hypertension patient showed increased perfusion TTP in hypoperfused regions in visual agreement with dynamic contrast-enhanced MRI, which improved postpulmonary endaterectomy surgery. Cystic fibrosis and chronic obstructive pulmonary disease patients showed a pattern of increased V-TTP and Q-TTP in regions of hypoventilation and decreased perfusion. Fractional ventilation flow-volume loops of the chronic obstructive pulmonary disease patient were smaller in comparison with the healthy volunteer, and showed regional differences in visual agreement with functional small airways disease and emphysema on CT.                                         Conclusions               This study shows the feasibility of phase-resolved functional lung imaging to gain quantitative information regarding regional lung perfusion and ventilation without the need for ultrafast imaging, which will be advantageous for future clinical translation. Magn Reson Med 79:2306--2314, 2018. {\copyright} 2017 International Society for Magnetic Resonance in Medicine.},
  langid = {english},
  file = {/home/temple/Sync/zotero/storage/XNKLGXPA/Voskrebenzev et al. - 2018 - Feasibility of quantitative regional ventilation a.pdf}
}

@article{wangImageQualityAssessment2004,
  title = {Image {{Quality Assessment}}: {{From Error Visibility}} to {{Structural Similarity}}},
  shorttitle = {Image {{Quality Assessment}}},
  author = {Wang, Z. and Bovik, A.C. and Sheikh, H.R. and Simoncelli, E.P.},
  year = {2004},
  month = apr,
  journal = {IEEE Transactions on Image Processing},
  volume = {13},
  number = {4},
  pages = {600--612},
  issn = {1057-7149},
  doi = {10.1109/TIP.2003.819861},
  urldate = {2024-05-01},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  langid = {english},
  file = {/home/temple/Sync/zotero/storage/TU648XT9/tip.2003.819861.pdf}
}

@article{zitovaImageRegistrationMethods2003,
  title = {Image Registration Methods: A Survey},
  shorttitle = {Image Registration Methods},
  author = {Zitov{\'a}, Barbara and Flusser, Jan},
  year = {2003},
  month = oct,
  journal = {Image and Vision Computing},
  volume = {21},
  number = {11},
  pages = {977--1000},
  issn = {02628856},
  doi = {10.1016/S0262-8856(03)00137-9},
  urldate = {2024-05-01},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/home/temple/Sync/zotero/storage/Q6EG9HYY/Zitov and Flusser - 2003 - Image registration methods a survey.pdf}
}

@article{zouReviewDeepLearningbased2022,
  title = {A Review of Deep Learning-Based Deformable Medical Image Registration},
  author = {Zou, Jing and Gao, Bingchen and Song, Youyi and Qin, Jing},
  year = {2022},
  month = dec,
  journal = {Frontiers in Oncology},
  volume = {12},
  pages = {1047215},
  issn = {2234-943X},
  doi = {10.3389/fonc.2022.1047215},
  urldate = {2024-05-03},
  abstract = {The alignment of images through deformable image registration is vital to clinical applications (e.g., atlas creation, image fusion, and tumor targeting in image-guided navigation systems) and is still a challenging problem. Recent progress in the field of deep learning has significantly advanced the performance of medical image registration. In this review, we present a comprehensive survey on deep learning-based deformable medical image registration methods. These methods are classified into five categories: Deep Iterative Methods, Supervised Methods, Unsupervised Methods, Weakly Supervised Methods, and Latest Methods. A detailed review of each category is provided with discussions about contributions, tasks, and inadequacies. We also provide statistical analysis for the selected papers from the point of view of image modality, the region of interest (ROI), evaluation metrics, and method categories. In addition, we summarize 33 publicly available datasets that are used for benchmarking the registration algorithms. Finally, the remaining challenges, future directions, and potential trends are discussed in our review.},
  file = {/home/temple/Sync/zotero/storage/9V9G22GY/Zou et al. - 2022 - A review of deep learning-based deformable medical.pdf}
}
